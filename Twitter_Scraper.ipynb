{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter-Scraper.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhbhupender1/ML-notebooks/blob/master/Twitter_Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw8smyrSXiRb",
        "colab_type": "text"
      },
      "source": [
        "# Twitter Scraper Online"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmoKnTQKU6Up",
        "colab_type": "text"
      },
      "source": [
        "This is an online version of the jonbakerfish/TweetScraper repository. It is one of the most complete Twitter scrapers.\n",
        "\n",
        "Credits: Jon https://github.com/jonbakerfish/TweetScraper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk5CkESnPshL",
        "colab_type": "text"
      },
      "source": [
        "Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kolpDd69FxS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "search = '@newsbreakapp' #comma separated keywords\n",
        "db = 'mongo' #mongo/mysql\n",
        "download_file = True\n",
        "store_in_drive = False\n",
        "clean_db_before_store = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz9XdqOnP5-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if store_in_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSU2MURLNPat",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhL-8qOJB_AO",
        "colab_type": "text"
      },
      "source": [
        "### Scraper Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIFQxxQXPpWN",
        "colab_type": "text"
      },
      "source": [
        "Verify first time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UouEHiYiCoRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "is_installed = !scrapy list\n",
        "if is_installed[0] == 'TweetScraper':\n",
        "  first_time = False\n",
        "else:\n",
        "  first_time = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_Nnq9wMPndW",
        "colab_type": "text"
      },
      "source": [
        "Install scraper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsY0ZsZgBLK0",
        "colab_type": "code",
        "outputId": "d9ef5643-66fe-44ba-b688-549072b6bdf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if first_time:\n",
        "  if store_in_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "  !git clone https://github.com/jonbakerfish/TweetScraper.git\n",
        "  %cd TweetScraper\n",
        "  !pip install -r requirements.txt  #add '--user' if you are not root\n",
        "  !scrapy list"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TweetScraper'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 256 (delta 14), reused 14 (delta 4), pack-reused 226\u001b[K\n",
            "Receiving objects: 100% (256/256), 55.60 KiB | 307.00 KiB/s, done.\n",
            "Resolving deltas: 100% (135/135), done.\n",
            "/content/TweetScraper\n",
            "Collecting scrapy (from -r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/4b/585e8e111ffb01466c59281f34febb13ad1a95d7fb3919fd57c33fc732a5/Scrapy-1.7.3-py2.py3-none-any.whl (234kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (3.9.0)\n",
            "Collecting mysql-connector (from -r requirements.txt (line 3))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/04/e40098f3730e75bbe36a338926f566ea803550a34fb50535499f4fc4787a/mysql-connector-2.2.9.tar.gz (11.9MB)\n",
            "\u001b[K     |████████████████████████████████| 11.9MB 22.4MB/s \n",
            "\u001b[?25hCollecting configparser (from -r requirements.txt (line 4))\n",
            "  Downloading https://files.pythonhosted.org/packages/7a/2a/95ed0501cf5d8709490b1d3a3f9b5cf340da6c433f896bbe9ce08dbe6785/configparser-4.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from scrapy->-r requirements.txt (line 1)) (1.12.0)\n",
            "Collecting cssselect>=0.9 (from scrapy->-r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Collecting queuelib (from scrapy->-r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from scrapy->-r requirements.txt (line 1)) (4.2.6)\n",
            "Collecting w3lib>=1.17.0 (from scrapy->-r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/45/1ba17c50a0bb16bd950c9c2b92ec60d40c8ebda9f3371ae4230c437120b6/w3lib-1.21.0-py2.py3-none-any.whl\n",
            "Collecting parsel>=1.5 (from scrapy->-r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/86/c8/fc5a2f9376066905dfcca334da2a25842aedfda142c0424722e7c497798b/parsel-1.5.2-py2.py3-none-any.whl\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy->-r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
            "Collecting pyOpenSSL (from scrapy->-r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/c8/ceb170d81bd3941cbeb9940fc6cc2ef2ca4288d0ca8929ea4db5905d904d/pyOpenSSL-19.0.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 25.5MB/s \n",
            "\u001b[?25hCollecting Twisted>=13.1.0; python_version != \"3.4\" (from scrapy->-r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/49/eb654da38b15285d1f594933eefff36ce03106356197dba28ee8f5721a79/Twisted-19.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 27.3MB/s \n",
            "\u001b[?25hCollecting service-identity (from scrapy->-r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n",
            "Collecting cryptography>=2.3 (from pyOpenSSL->scrapy->-r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/18/c6557f63a6abde34707196fb2cad1c6dc0dbff25a200d5044922496668a4/cryptography-2.7-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 32.7MB/s \n",
            "\u001b[?25hCollecting Automat>=0.3.0 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy->-r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/86/14c16bb98a5a3542ed8fed5d74fb064a902de3bdd98d6584b34553353c45/Automat-0.7.0-py2.py3-none-any.whl\n",
            "Collecting constantly>=15.1 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy->-r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
            "Collecting incremental>=16.10.1 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy->-r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy->-r requirements.txt (line 1)) (19.2.0)\n",
            "Collecting PyHamcrest>=1.9.0 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy->-r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d5/d37fd731b7d0e91afcc84577edeccf4638b4f9b82f5ffe2f8b62e2ddc609/PyHamcrest-1.9.0-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 26.7MB/s \n",
            "\u001b[?25hCollecting zope.interface>=4.4.2 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy->-r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/17/1d198a6aaa9aa4590862fe3d3a2ed7dd808050cab4eebe8a2f2f813c1376/zope.interface-4.6.0-cp36-cp36m-manylinux1_x86_64.whl (167kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 51.3MB/s \n",
            "\u001b[?25hCollecting hyperlink>=17.1.1 (from Twisted>=13.1.0; python_version != \"3.4\"->scrapy->-r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/91/e916ca10a2de1cb7101a9b24da546fb90ee14629e23160086cf3361c4fb8/hyperlink-19.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity->scrapy->-r requirements.txt (line 1)) (0.4.7)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity->scrapy->-r requirements.txt (line 1)) (0.2.6)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.3->pyOpenSSL->scrapy->-r requirements.txt (line 1)) (1.12.3)\n",
            "Collecting asn1crypto>=0.21.0 (from cryptography>=2.3->pyOpenSSL->scrapy->-r requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/1e/fb0e487b5229e5fb7b15c6d00b4e8082a3414fe62b1da4c9a905b106e672/asn1crypto-1.1.0-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 44.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from PyHamcrest>=1.9.0->Twisted>=13.1.0; python_version != \"3.4\"->scrapy->-r requirements.txt (line 1)) (41.2.0)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=13.1.0; python_version != \"3.4\"->scrapy->-r requirements.txt (line 1)) (2.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.3->pyOpenSSL->scrapy->-r requirements.txt (line 1)) (2.19)\n",
            "Building wheels for collected packages: mysql-connector, PyDispatcher\n",
            "  Building wheel for mysql-connector (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysql-connector: filename=mysql_connector-2.2.9-cp36-cp36m-linux_x86_64.whl size=247949 sha256=18810f4bbb06e78bf0a887a9f7406d6c33f06198f3bbb12de68b5eacb1bdf1d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/83/a1/f8b6d4bb1bd6208bbde1608bbfa7557504bed9eaf2ecf8c175\n",
            "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-cp36-none-any.whl size=11516 sha256=b5d5ac86bcf7d1e4008a615d9d7ac6935609360e207ae0a22482a2cf61aba2e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
            "Successfully built mysql-connector PyDispatcher\n",
            "Installing collected packages: cssselect, queuelib, w3lib, parsel, PyDispatcher, asn1crypto, cryptography, pyOpenSSL, Automat, constantly, incremental, PyHamcrest, zope.interface, hyperlink, Twisted, service-identity, scrapy, mysql-connector, configparser\n",
            "Successfully installed Automat-0.7.0 PyDispatcher-2.0.5 PyHamcrest-1.9.0 Twisted-19.7.0 asn1crypto-1.1.0 configparser-4.0.2 constantly-15.1.0 cryptography-2.7 cssselect-1.1.0 hyperlink-19.0.0 incremental-17.5.0 mysql-connector-2.2.9 parsel-1.5.2 pyOpenSSL-19.0.0 queuelib-1.5.0 scrapy-1.7.3 service-identity-18.1.0 w3lib-1.21.0 zope.interface-4.6.0\n",
            "TweetScraper\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvguVXmaXZUu",
        "colab_type": "text"
      },
      "source": [
        "### MongoDB Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t199z5ZbXJtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if first_time and db == 'mongo':\n",
        "  !apt install mongodb\n",
        "  !service mongodb start\n",
        "  !sed -i \"s/TweetScraper.pipelines.SaveToFilePipeline/TweetScraper.pipelines.SaveToMongoPipeline/\" /content/TweetScraper/TweetScraper/settings.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gl9aVWgOyAf",
        "colab_type": "text"
      },
      "source": [
        "Clean the database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHXNBTtbOw9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not first_time and clean_db_before_store and db == 'mongo':\n",
        "  !mongo --eval \"print(db.getSiblingDB('TweetScraper').getCollection('tweet').count());print(db.getSiblingDB('TweetScraper').getCollection('user').count());\"\n",
        "  !mongo --eval \"db.getSiblingDB('TweetScraper').getCollection('tweet').remove({});db.getSiblingDB('TweetScraper').getCollection('user').remove({})\"\n",
        "  !mongo --eval \"print(db.getSiblingDB('TweetScraper').getCollection('tweet').count());print(db.getSiblingDB('TweetScraper').getCollection('user').count());\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuFSpPcoXqmf",
        "colab_type": "text"
      },
      "source": [
        "### MySQL Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LxsEKXZ8Bus",
        "colab_type": "text"
      },
      "source": [
        "Install MySQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnWArgYMXxo-",
        "colab_type": "code",
        "outputId": "634421bb-5c2c-4ded-e3cd-8d32a6e25768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if first_time and db == 'mysql':\n",
        "  !apt install mysql-server\n",
        "  !service mysql start\n",
        "  !mysql -u root -e \"create database TweetScraper\";\n",
        "  !sed -i \"s/TweetScraper.pipelines.SaveToFilePipeline/TweetScraper.pipelines.SavetoMySQLPipeline/\" /content/TweetScraper/TweetScraper/settings.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcgi-fast-perl libcgi-pm-perl libencode-locale-perl libevent-core-2.1-6\n",
            "  libfcgi-perl libhtml-parser-perl libhtml-tagset-perl libhtml-template-perl\n",
            "  libhttp-date-perl libhttp-message-perl libio-html-perl\n",
            "  liblwp-mediatypes-perl libtimedate-perl liburi-perl mysql-client-5.7\n",
            "  mysql-client-core-5.7 mysql-server-5.7 mysql-server-core-5.7 psmisc\n",
            "Suggested packages:\n",
            "  libdata-dump-perl libipc-sharedcache-perl libwww-perl mailx tinyca\n",
            "The following NEW packages will be installed:\n",
            "  libcgi-fast-perl libcgi-pm-perl libencode-locale-perl libevent-core-2.1-6\n",
            "  libfcgi-perl libhtml-parser-perl libhtml-tagset-perl libhtml-template-perl\n",
            "  libhttp-date-perl libhttp-message-perl libio-html-perl\n",
            "  liblwp-mediatypes-perl libtimedate-perl liburi-perl mysql-client-5.7\n",
            "  mysql-client-core-5.7 mysql-server mysql-server-5.7 mysql-server-core-5.7\n",
            "  psmisc\n",
            "0 upgraded, 20 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 21.1 MB of archives.\n",
            "After this operation, 162 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 mysql-client-core-5.7 amd64 5.7.27-0ubuntu0.18.04.1 [7,040 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 mysql-client-5.7 amd64 5.7.27-0ubuntu0.18.04.1 [2,302 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 mysql-server-core-5.7 amd64 5.7.27-0ubuntu0.18.04.1 [7,779 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 psmisc amd64 23.1-1ubuntu0.1 [52.5 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libevent-core-2.1-6 amd64 2.1.8-stable-4build1 [85.9 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 mysql-server-5.7 amd64 5.7.27-0ubuntu0.18.04.1 [3,196 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-pm-perl all 4.38-1 [185 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfcgi-perl amd64 0.78-2build1 [32.8 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-fast-perl all 1:2.13-1 [9,940 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-template-perl all 2.97-1 [59.0 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 mysql-server all 5.7.27-0ubuntu0.18.04.1 [9,948 B]\n",
            "Fetched 21.1 MB in 26s (816 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package mysql-client-core-5.7.\n",
            "(Reading database ... 131183 files and directories currently installed.)\n",
            "Preparing to unpack .../00-mysql-client-core-5.7_5.7.27-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking mysql-client-core-5.7 (5.7.27-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package mysql-client-5.7.\n",
            "Preparing to unpack .../01-mysql-client-5.7_5.7.27-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking mysql-client-5.7 (5.7.27-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package mysql-server-core-5.7.\n",
            "Preparing to unpack .../02-mysql-server-core-5.7_5.7.27-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking mysql-server-core-5.7 (5.7.27-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package psmisc.\n",
            "Preparing to unpack .../03-psmisc_23.1-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking psmisc (23.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libevent-core-2.1-6:amd64.\n",
            "Preparing to unpack .../04-libevent-core-2.1-6_2.1.8-stable-4build1_amd64.deb ...\n",
            "Unpacking libevent-core-2.1-6:amd64 (2.1.8-stable-4build1) ...\n",
            "Selecting previously unselected package mysql-server-5.7.\n",
            "Preparing to unpack .../05-mysql-server-5.7_5.7.27-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking mysql-server-5.7 (5.7.27-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libhtml-tagset-perl.\n",
            "Preparing to unpack .../06-libhtml-tagset-perl_3.20-3_all.deb ...\n",
            "Unpacking libhtml-tagset-perl (3.20-3) ...\n",
            "Selecting previously unselected package liburi-perl.\n",
            "Preparing to unpack .../07-liburi-perl_1.73-1_all.deb ...\n",
            "Unpacking liburi-perl (1.73-1) ...\n",
            "Selecting previously unselected package libhtml-parser-perl.\n",
            "Preparing to unpack .../08-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n",
            "Unpacking libhtml-parser-perl (3.72-3build1) ...\n",
            "Selecting previously unselected package libcgi-pm-perl.\n",
            "Preparing to unpack .../09-libcgi-pm-perl_4.38-1_all.deb ...\n",
            "Unpacking libcgi-pm-perl (4.38-1) ...\n",
            "Selecting previously unselected package libfcgi-perl.\n",
            "Preparing to unpack .../10-libfcgi-perl_0.78-2build1_amd64.deb ...\n",
            "Unpacking libfcgi-perl (0.78-2build1) ...\n",
            "Selecting previously unselected package libcgi-fast-perl.\n",
            "Preparing to unpack .../11-libcgi-fast-perl_1%3a2.13-1_all.deb ...\n",
            "Unpacking libcgi-fast-perl (1:2.13-1) ...\n",
            "Selecting previously unselected package libencode-locale-perl.\n",
            "Preparing to unpack .../12-libencode-locale-perl_1.05-1_all.deb ...\n",
            "Unpacking libencode-locale-perl (1.05-1) ...\n",
            "Selecting previously unselected package libhtml-template-perl.\n",
            "Preparing to unpack .../13-libhtml-template-perl_2.97-1_all.deb ...\n",
            "Unpacking libhtml-template-perl (2.97-1) ...\n",
            "Selecting previously unselected package libtimedate-perl.\n",
            "Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\n",
            "Selecting previously unselected package libhttp-date-perl.\n",
            "Preparing to unpack .../15-libhttp-date-perl_6.02-1_all.deb ...\n",
            "Unpacking libhttp-date-perl (6.02-1) ...\n",
            "Selecting previously unselected package libio-html-perl.\n",
            "Preparing to unpack .../16-libio-html-perl_1.001-1_all.deb ...\n",
            "Unpacking libio-html-perl (1.001-1) ...\n",
            "Selecting previously unselected package liblwp-mediatypes-perl.\n",
            "Preparing to unpack .../17-liblwp-mediatypes-perl_6.02-1_all.deb ...\n",
            "Unpacking liblwp-mediatypes-perl (6.02-1) ...\n",
            "Selecting previously unselected package libhttp-message-perl.\n",
            "Preparing to unpack .../18-libhttp-message-perl_6.14-1_all.deb ...\n",
            "Unpacking libhttp-message-perl (6.14-1) ...\n",
            "Selecting previously unselected package mysql-server.\n",
            "Preparing to unpack .../19-mysql-server_5.7.27-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking mysql-server (5.7.27-0ubuntu0.18.04.1) ...\n",
            "Setting up libhtml-tagset-perl (3.20-3) ...\n",
            "Setting up libevent-core-2.1-6:amd64 (2.1.8-stable-4build1) ...\n",
            "Setting up psmisc (23.1-1ubuntu0.1) ...\n",
            "Setting up libencode-locale-perl (1.05-1) ...\n",
            "Setting up mysql-server-core-5.7 (5.7.27-0ubuntu0.18.04.1) ...\n",
            "Setting up libtimedate-perl (2.3000-2) ...\n",
            "Setting up libio-html-perl (1.001-1) ...\n",
            "Setting up liblwp-mediatypes-perl (6.02-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Setting up liburi-perl (1.73-1) ...\n",
            "Processing triggers for systemd (237-3ubuntu10.25) ...\n",
            "Setting up libhtml-parser-perl (3.72-3build1) ...\n",
            "Setting up libcgi-pm-perl (4.38-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up mysql-client-core-5.7 (5.7.27-0ubuntu0.18.04.1) ...\n",
            "Setting up libfcgi-perl (0.78-2build1) ...\n",
            "Setting up libhttp-date-perl (6.02-1) ...\n",
            "Setting up libhtml-template-perl (2.97-1) ...\n",
            "Setting up libcgi-fast-perl (1:2.13-1) ...\n",
            "Setting up libhttp-message-perl (6.14-1) ...\n",
            "Setting up mysql-client-5.7 (5.7.27-0ubuntu0.18.04.1) ...\n",
            "Setting up mysql-server-5.7 (5.7.27-0ubuntu0.18.04.1) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of stop.\n",
            "update-alternatives: using /etc/mysql/mysql.cnf to provide /etc/mysql/my.cnf (my.cnf) in auto mode\n",
            "Renaming removed key_buffer and myisam-recover options (if present)\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/mysql.service → /lib/systemd/system/mysql.service.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up mysql-server (5.7.27-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for systemd (237-3ubuntu10.25) ...\n",
            " * Starting MySQL database server mysqld\n",
            "No directory, logging in with HOME=/\n",
            "   ...done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzWOEvI779gS",
        "colab_type": "text"
      },
      "source": [
        "Show the default credentials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNKbiwY4ddbb",
        "colab_type": "code",
        "outputId": "50cabffc-bc30-4377-bbab-33d399e0e6ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "if first_time and db == 'mysql':\n",
        "  !cat /etc/mysql/debian.cnf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Automatically generated for Debian scripts. DO NOT TOUCH!\n",
            "[client]\n",
            "host     = localhost\n",
            "user     = debian-sys-maint\n",
            "password = us5TMM6BOSoz4oTW\n",
            "socket   = /var/run/mysqld/mysqld.sock\n",
            "[mysql_upgrade]\n",
            "host     = localhost\n",
            "user     = debian-sys-maint\n",
            "password = us5TMM6BOSoz4oTW\n",
            "socket   = /var/run/mysqld/mysqld.sock\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCrPVxs3E8pF",
        "colab_type": "code",
        "outputId": "f33356b9-eafe-4c26-e402-9eb0ee746844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "if first_time and db == 'mysql':\n",
        "  import configparser\n",
        "  config = configparser.ConfigParser()\n",
        "  config.read('/etc/mysql/debian.cnf')\n",
        "  db_user = config['client']['user']\n",
        "  db_pass = config['client']['password']\n",
        "  print(db_user)\n",
        "  print(db_pass)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "debian-sys-maint\n",
            "us5TMM6BOSoz4oTW\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQmJ_fix75C4",
        "colab_type": "text"
      },
      "source": [
        "Set the previously displayed credentials"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0plB_Fv7RVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if first_time and db == 'mysql':\n",
        "  !sed -i 's/MYSQL_USER   = .*/MYSQL_USER   = \"{db_user}\"/' /content/TweetScraper/TweetScraper/settings.py\n",
        "  !sed -i 's/MYSQL_PWD    = .*/MYSQL_PWD    = \"{db_pass}\"/' /content/TweetScraper/TweetScraper/settings.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9EDoW5nPBfc",
        "colab_type": "text"
      },
      "source": [
        "Clean the database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K3jg8weOs9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not first_time and clean_db_before_store and db == 'mysql':\n",
        "  !mysql TweetScraper -e \"DELETE * from scraper\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6dy8H87XyTE",
        "colab_type": "text"
      },
      "source": [
        "# Run the Scraper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LgHADJ0GMFx",
        "colab_type": "code",
        "outputId": "be6b6b22-321d-4cd0-9695-71ddc1440feb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!scrapy crawl TweetScraper -a query=\"{search}\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-16 02:32:21 [scrapy.utils.log] INFO: Scrapy 1.7.3 started (bot: TweetScraper)\n",
            "2019-10-16 02:32:21 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.7.0, Python 3.6.8 (default, Jan 14 2019, 11:02:34) - [GCC 8.0.1 20180414 (experimental) [trunk revision 259383]], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Linux-4.14.137+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2019-10-16 02:32:21 [scrapy.crawler] INFO: Overridden settings: {'BOT_NAME': 'TweetScraper', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'TweetScraper.spiders', 'SPIDER_MODULES': ['TweetScraper.spiders'], 'USER_AGENT': 'TweetScraper'}\n",
            "2019-10-16 02:32:21 [scrapy.extensions.telnet] INFO: Telnet Password: 49dcae699560c09e\n",
            "2019-10-16 02:32:21 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2019-10-16 02:32:21 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2019-10-16 02:32:21 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2019-10-16 02:32:21 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "['TweetScraper.pipelines.SavetoMySQLPipeline']\n",
            "2019-10-16 02:32:21 [scrapy.core.engine] INFO: Spider opened\n",
            "2019-10-16 02:32:21 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2019-10-16 02:32:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2019-10-16 02:32:22 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:23 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xA7\\xD9\\x84\\xD8\\xA8...' for column 'text' at row 1\n",
            "2019-10-16 02:32:23 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:23 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:24 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:24 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:24 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:25 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:25 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:25 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:25 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:25 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:26 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:27 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE2\\x80\\x8BPre...' for column 'text' at row 1\n",
            "2019-10-16 02:32:27 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:28 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:31 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:31 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:32 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE6\\x9D\\xA5\\xE8\\x87\\xAA...' for column 'text' at row 1\n",
            "2019-10-16 02:32:33 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE6\\x9D\\xA5\\xE8\\x87\\xAA...' for column 'text' at row 1\n",
            "2019-10-16 02:32:34 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:35 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:35 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:35 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xEB\\x8B\\x98\\xEC\\x9D\\xB4...' for column 'text' at row 1\n",
            "2019-10-16 02:32:36 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:36 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:40 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:41 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE6\\x9D\\xA5\\xE8\\x87\\xAA...' for column 'text' at row 1\n",
            "2019-10-16 02:32:42 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:44 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:44 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:45 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD1\\x81 \\xD0\\xBF\\xD0...' for column 'text' at row 1\n",
            "2019-10-16 02:32:45 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD1\\x81 \\xD0\\xBF\\xD0...' for column 'text' at row 1\n",
            "2019-10-16 02:32:45 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD1\\x81 \\xD0\\xBF\\xD0...' for column 'text' at row 1\n",
            "2019-10-16 02:32:45 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD1\\x81 \\xD0\\xBF\\xD0...' for column 'text' at row 1\n",
            "2019-10-16 02:32:45 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD1\\x81 \\xD0\\xBF\\xD0...' for column 'text' at row 1\n",
            "2019-10-16 02:32:45 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD1\\x81 \\xD0\\xBF\\xD0...' for column 'text' at row 1\n",
            "2019-10-16 02:32:45 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD1\\x81 \\xD0\\xBF\\xD0...' for column 'text' at row 1\n",
            "2019-10-16 02:32:46 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:47 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:47 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:47 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:50 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:51 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:51 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:51 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:51 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:52 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:54 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:56 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xC4\\xB1l\\xC4\\xB1\\xC4...' for column 'text' at row 1\n",
            "2019-10-16 02:32:59 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD1\\x81 \\xD0\\xBF\\xD0...' for column 'text' at row 1\n",
            "2019-10-16 02:32:59 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:32:59 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:01 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD1\\x81 \\xD0\\xBF\\xD0...' for column 'text' at row 1\n",
            "2019-10-16 02:33:02 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:06 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:13 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:15 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:15 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:18 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:19 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:21 [scrapy.extensions.logstats] INFO: Crawled 92 pages (at 92 pages/min), scraped 1825 items (at 1825 items/min)\n",
            "2019-10-16 02:33:22 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:23 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:26 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:27 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:29 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD1\\x81 \\xD0\\xBF\\xD0...' for column 'text' at row 1\n",
            "2019-10-16 02:33:29 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD1\\x81 \\xD0\\xBF\\xD0...' for column 'text' at row 1\n",
            "2019-10-16 02:33:29 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:29 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:33 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:35 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:35 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:36 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xA7\\xD8\\xB2 \\xD8...' for column 'text' at row 1\n",
            "2019-10-16 02:33:37 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:39 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE3\\x81\\x8B\\xE3\\x82\\x89' for column 'text' at row 1\n",
            "2019-10-16 02:33:40 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:41 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:45 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:46 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:46 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:47 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:50 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:51 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:52 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:53 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:54 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:54 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:57 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:33:58 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:34:00 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:34:00 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:34:01 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:34:01 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:34:03 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:34:05 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:34:08 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:34:08 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:34:11 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:34:14 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:34:15 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:34:15 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:16 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:34:16 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:34:16 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE2\\x80\\x8B\\xE2\\x80\\x8B...' for column 'text' at row 1\n",
            "2019-10-16 02:34:18 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:34:20 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:20 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:20 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD7\\x93\\xD7\\xA8\\xD7\\x9A...' for column 'text' at row 1\n",
            "2019-10-16 02:34:21 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:34:21 [scrapy.extensions.logstats] INFO: Crawled 180 pages (at 88 pages/min), scraped 3589 items (at 1764 items/min)\n",
            "2019-10-16 02:34:21 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:34:22 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE0\\xB8\\x82\\xE0\\xB9\\x89...' for column 'text' at row 1\n",
            "2019-10-16 02:34:22 [TweetScraper.pipelines] INFO: Data too long for column 'text' at row 1\n",
            "2019-10-16 02:34:23 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE1\\x80\\xBB\\xE1\\x80\\x80...' for column 'text' at row 1\n",
            "2019-10-16 02:34:29 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD0\\xBF\\xD1\\x80\\xD0\\xB5...' for column 'text' at row 1\n",
            "2019-10-16 02:34:31 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:31 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xA7\\xD8\\xB2 \\xD8...' for column 'text' at row 1\n",
            "2019-10-16 02:34:32 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD0\\xBE\\xD1\\x82 @...' for column 'text' at row 1\n",
            "2019-10-16 02:34:32 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD0\\xBE\\xD1\\x82 @...' for column 'text' at row 1\n",
            "2019-10-16 02:34:32 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:32 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:32 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:32 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:32 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:32 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE1\\x80\\x81\\xE1\\x80\\xBB...' for column 'text' at row 1\n",
            "2019-10-16 02:34:33 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE0\\xB8\\x9C\\xE0\\xB9\\x88...' for column 'text' at row 1\n",
            "2019-10-16 02:34:34 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:34 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE0\\xA4\\xAD\\xE0\\xA4\\x97...' for column 'text' at row 1\n",
            "2019-10-16 02:34:34 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:34 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE0\\xA4\\xAF\\xE0\\xA4\\xB9...' for column 'text' at row 1\n",
            "2019-10-16 02:34:34 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:40 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xA7\\xD8\\xB2 \\xD8...' for column 'text' at row 1\n",
            "2019-10-16 02:34:41 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE0\\xB8\\x82\\xE0\\xB9\\x89...' for column 'text' at row 1\n",
            "2019-10-16 02:34:41 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE0\\xB8\\x82\\xE0\\xB9\\x89...' for column 'text' at row 1\n",
            "2019-10-16 02:34:41 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE0\\xB8\\x82\\xE0\\xB9\\x89...' for column 'text' at row 1\n",
            "2019-10-16 02:34:41 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:42 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:43 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:44 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:45 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:46 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE0\\xB8\\x82\\xE0\\xB9\\x89...' for column 'text' at row 1\n",
            "2019-10-16 02:34:46 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:46 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:47 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE0\\xA4\\xB8\\xE0\\xA4\\xB0...' for column 'text' at row 1\n",
            "2019-10-16 02:34:49 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE0\\xA4\\xA6\\xE0\\xA5\\x87...' for column 'text' at row 1\n",
            "2019-10-16 02:34:49 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:49 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xA7\\xD8\\xB2 \\xD8...' for column 'text' at row 1\n",
            "2019-10-16 02:34:49 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:52 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:53 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:55 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:55 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:57 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:57 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:58 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:34:59 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE0\\xB8\\x82\\xE0\\xB9\\x89...' for column 'text' at row 1\n",
            "2019-10-16 02:34:59 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xA7\\xD8\\xB2 \\xD8...' for column 'text' at row 1\n",
            "2019-10-16 02:35:02 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xB9\\xD8\\xA8\\xD8\\xB1...' for column 'text' at row 1\n",
            "2019-10-16 02:35:02 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE0\\xA6\\x8F\\xE0\\xA6\\xB0...' for column 'text' at row 1\n",
            "2019-10-16 02:35:03 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD8\\xA7\\xD8\\xB2 \\xD8...' for column 'text' at row 1\n",
            "2019-10-16 02:35:21 [scrapy.extensions.logstats] INFO: Crawled 277 pages (at 97 pages/min), scraped 5513 items (at 1924 items/min)\n",
            "2019-10-16 02:36:21 [scrapy.extensions.logstats] INFO: Crawled 378 pages (at 101 pages/min), scraped 7262 items (at 1749 items/min)\n",
            "2019-10-16 02:37:21 [scrapy.extensions.logstats] INFO: Crawled 477 pages (at 99 pages/min), scraped 9123 items (at 1861 items/min)\n",
            "2019-10-16 02:38:21 [scrapy.extensions.logstats] INFO: Crawled 572 pages (at 95 pages/min), scraped 11043 items (at 1920 items/min)\n",
            "2019-10-16 02:39:11 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xD0\\xA2\\xD0\\x9E\\xD0\\x9F...' for column 'text' at row 1\n",
            "2019-10-16 02:39:21 [scrapy.extensions.logstats] INFO: Crawled 663 pages (at 91 pages/min), scraped 12863 items (at 1820 items/min)\n",
            "2019-10-16 02:39:52 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xE3\\x80\\x9090s...' for column 'text' at row 1\n",
            "2019-10-16 02:40:07 [TweetScraper.pipelines] INFO: Incorrect string value: '\\xCE\\xB17s t...' for column 'text' at row 1\n",
            "2019-10-16 02:40:21 [scrapy.extensions.logstats] INFO: Crawled 755 pages (at 92 pages/min), scraped 14703 items (at 1840 items/min)\n",
            "2019-10-16 02:41:21 [scrapy.extensions.logstats] INFO: Crawled 848 pages (at 93 pages/min), scraped 16563 items (at 1860 items/min)\n",
            "2019-10-16 02:41:43 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2019-10-16 02:41:43 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 792205,\n",
            " 'downloader/request_count': 881,\n",
            " 'downloader/request_method_count/GET': 881,\n",
            " 'downloader/response_bytes': 9990827,\n",
            " 'downloader/response_count': 881,\n",
            " 'downloader/response_status_count/200': 881,\n",
            " 'dupefilter/filtered': 1,\n",
            " 'elapsed_time_seconds': 561.43627,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2019, 10, 16, 2, 41, 43, 206143),\n",
            " 'item_scraped_count': 17186,\n",
            " 'log_count/INFO': 181,\n",
            " 'memusage/max': 95199232,\n",
            " 'memusage/startup': 95199232,\n",
            " 'request_depth_max': 881,\n",
            " 'response_received_count': 881,\n",
            " 'scheduler/dequeued': 881,\n",
            " 'scheduler/dequeued/memory': 881,\n",
            " 'scheduler/enqueued': 881,\n",
            " 'scheduler/enqueued/memory': 881,\n",
            " 'start_time': datetime.datetime(2019, 10, 16, 2, 32, 21, 769873)}\n",
            "2019-10-16 02:41:43 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngJi7UohNlmf",
        "colab_type": "text"
      },
      "source": [
        "# Store the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrBfX7I0X811",
        "colab_type": "text"
      },
      "source": [
        "## MongoDB save to CSV (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bpn3EDDPYKsE",
        "colab_type": "text"
      },
      "source": [
        "Get header names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RPnav0MM5op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if first_time and db == 'mongo':\n",
        "  !mongo --eval \"var row = db.getSiblingDB('TweetScraper').getCollection('tweet').findOne();for (var i in row) {print(i);}\" > output.txt\n",
        "  !tail -n +4 output.txt > fields.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5leubSIYRsb",
        "colab_type": "text"
      },
      "source": [
        "Number of records"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlE-LTLlxCpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if db == 'mongo':\n",
        "  !mongo --eval \"print(db.getSiblingDB('TweetScraper').getCollection('tweet').count());print(db.getSiblingDB('TweetScraper').getCollection('user').count());\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1VtkIgTYfiB",
        "colab_type": "text"
      },
      "source": [
        "Export file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ8FRx1RHCc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if db == 'mongo':\n",
        "  !mongoexport -d TweetScraper -c tweet --type csv --fieldFile=fields.txt -o tweet.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLv6wAd98wXA",
        "colab_type": "text"
      },
      "source": [
        "## MySQL save to CSV (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IncF4T58PbeZ",
        "colab_type": "text"
      },
      "source": [
        "Number of records"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NHA1Zoq_tMq",
        "colab_type": "code",
        "outputId": "60480a47-c70b-4a9a-e0f8-606c39004fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "if db == 'mysql':\n",
        "  !mysql TweetScraper -e \"select COUNT(*) from scraper\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+\n",
            "| COUNT(*) |\n",
            "+----------+\n",
            "|    17024 |\n",
            "+----------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmUO0WQ8PZst",
        "colab_type": "text"
      },
      "source": [
        "Export file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2B6MEcz84Gk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if db == 'mysql':\n",
        "  !mysql TweetScraper -e \"select * from scraper\" -B | sed \"s/'/\\'/;s/\\t/\\\",\\\"/g;s/^/\\\"/;s/$/\\\"/;s/\\n//g\" > tweet.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP7NXBG6aoMs",
        "colab_type": "text"
      },
      "source": [
        "Export sql"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OBPYb_JWtAw",
        "colab_type": "code",
        "outputId": "453bc615-e4a8-432d-874d-3670b11ab55d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "if db == 'mysql':\n",
        "  !mongodump TweetScraper scraper -o tweet.sql"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-10-16T02:30:10.528+0000\tpositional arguments not allowed: [TweetScraper scraper]\n",
            "2019-10-16T02:30:10.528+0000\ttry 'mongodump --help' for more information\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAsZGpDXZADc",
        "colab_type": "text"
      },
      "source": [
        "# Download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPeeizp7G2-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = search.replace(' ', '_')\n",
        "filename = filename.replace(',', '__')\n",
        "!mv tweet.csv {filename}.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1G1dgcUZO0s",
        "colab_type": "text"
      },
      "source": [
        "## Download to Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggAH3vX8tQFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if store_in_drive:\n",
        "  !cp tweet.csv /content/drive/My\\ Drive/{filename}.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo9zH87aZIGm",
        "colab_type": "text"
      },
      "source": [
        "## Download to local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUl1XI-ksnYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if download_file:\n",
        "  import time\n",
        "  from google.colab import files\n",
        "  time.sleep(2)\n",
        "  try:\n",
        "    files.download(filename + '.csv')\n",
        "  except:\n",
        "    print('Please retry')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}