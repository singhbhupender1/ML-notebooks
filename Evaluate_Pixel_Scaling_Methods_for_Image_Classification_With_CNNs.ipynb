{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Evaluate Pixel Scaling Methods for Image Classification With CNNs.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhbhupender1/ML-notebooks/blob/master/Evaluate_Pixel_Scaling_Methods_for_Image_Classification_With_CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwK8ihleKAWQ",
        "colab_type": "text"
      },
      "source": [
        "##Procedure for Choosing a Pixel Scaling Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuQigeWMKT0f",
        "colab_type": "text"
      },
      "source": [
        "**Given a new image classification task, what pixel scaling methods should be used?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucjd0Gg6KYnO",
        "colab_type": "text"
      },
      "source": [
        "**This can be achieved using the following process:**\n",
        "\n",
        "**Step 1: Choose Dataset. This may be the entire training dataset or a small subset. The idea is to complete the experiments quickly and get a result.**\n",
        "\n",
        "**Step 2: Choose Model. Design a model that is skillful, but not necessarily the best model for the problem. Some parallel prototyping of models may be required.**\n",
        "\n",
        "**Step 3: Choose Pixel Scaling Methods. List 3-5 data preparation schemes for evaluation of your problem.**\n",
        "\n",
        "**Step 4: Run Experiment. Run the experiments in such a way that the results are robust and representative, ideally repeat each experiment multiple times.**\n",
        "\n",
        "**Step 5: Analyze Results. Compare methods both in terms of the speed of learning and mean performance across repeated experiments.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjtDtAhTMk7F",
        "colab_type": "text"
      },
      "source": [
        "**you are looking for a signal that one data preparation scheme for your images is clearly better than the others; if this is not the case for your dataset, then the simplest (least computationally complex) technique should be used, such as pixel normalization.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJtlCOG4MrGs",
        "colab_type": "text"
      },
      "source": [
        "**A clear signal of a superior pixel scaling method may be seen in one of two ways:**\n",
        "\n",
        "**Faster Learning. Learning curves clearly show that a model learns faster with a given data preparation scheme.**\n",
        "\n",
        "**Better Accuracy. Mean model performance clearly shows better accuracy with a given data preparation scheme.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHJ5F4ebM1Ha",
        "colab_type": "text"
      },
      "source": [
        "##Step 1. Choose Dataset: MNIST Image Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOw1QcOdM-so",
        "colab_type": "text"
      },
      "source": [
        "**The MNIST problem, or MNIST for short, is an image classification problem comprised of 70,000 images of handwritten digits.**\n",
        "\n",
        "**The goal of the problem is to classify a given image of a handwritten digit as an integer from 0 to 9. As such, it is a multiclass image classification problem.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWbEOw63NRHV",
        "colab_type": "text"
      },
      "source": [
        "**This dataset is provided as part of the Keras library and can be automatically downloaded (if needed) and loaded into memory by a call to the keras.datasets.mnist.load_data() function.**\n",
        "\n",
        "**The function returns two tuples: one for the training inputs and outputs and one for the test inputs and outputs. For example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWS-jaeZAogf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "a7374881-0de1-48d2-c47e-4898e96577ce"
      },
      "source": [
        "#load and summarize the MNIST dataset\n",
        "from keras.datasets import mnist\n",
        "#load the dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "#summarize the dataset shape\n",
        "print('Train', train_images.shape, train_labels.shape)\n",
        "print('Test', test_images.shape, test_labels.shape)\n",
        "#summerize pixels shape\n",
        "print('Train', train_images.min(), train_images.max(), train_images.mean(), train_images.std())\n",
        "print('Train', test_images.min(), test_images.max(), test_images.mean(), test_images.std())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train (60000, 28, 28) (60000,)\n",
            "Test (10000, 28, 28) (10000,)\n",
            "Train 0 255 33.318421449829934 78.56748998339798\n",
            "Train 0 255 33.791224489795916 79.17246322228644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMaw7cwGRQZ4",
        "colab_type": "text"
      },
      "source": [
        "##Step 2. Choose Model: Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6qEYsPjRYea",
        "colab_type": "text"
      },
      "source": [
        "**We will use a convolutional neural network model to evaluate the different pixel scaling methods.**\n",
        "\n",
        "**A CNN is expected to perform very well on this problem, although the model chosen for this experiment does not have to perform well or best for the problem. Instead, it must be skillful (better than random) and must allow the impact of different data preparation schemes to be differentiated in terms of speed of learning and/or model performance.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgcoBheJRhJ2",
        "colab_type": "text"
      },
      "source": [
        "**We will demonstrate the baseline model on the MNIST problem.**\n",
        "\n",
        "**First, the dataset must be loaded and the shape of the train and test dataset expanded to add a channel dimension, set to one as we only have a single black and white channel.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGkf56mHO37q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the datast\n",
        "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
        "#reshape dataset to have a single channel\n",
        "width, height, channels = trainX.shape[1], trainX.shape[2], 1\n",
        "trainX = trainX.reshape((trainX.shape[0], width, height, channels))\n",
        "testX = testX.reshape((testX.shape[0], width, height, channels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-qSqvL0TqJ2",
        "colab_type": "text"
      },
      "source": [
        "**Next, we will normalize the pixel values for this example and one hot encode the target values, required for multiclass classification.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDQfsfjcTmVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "#normalize pixel values\n",
        "trainX = trainX.astype('float32') / 255\n",
        "testX = testX.astype('float32') /255\n",
        "#one hot encode target values\n",
        "trainY = to_categorical(trainY)\n",
        "testY = to_categorical(testY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWUnuteRU3OH",
        "colab_type": "text"
      },
      "source": [
        "**The model is defined as a convolutional layer followed by a max pooling layer; this combination is repeated again, then the filter maps are flattened, interpreted by a fully connected layer and followed by an output layer.**\n",
        "\n",
        "**The ReLU activation function is used for hidden layers and the softmax activation function is used for the output layer. Enough filter maps and nodes are specified to provide sufficient capacity to learn the problem.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_apHtmKVRXV",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, channels)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_YmKb-eViRk",
        "colab_type": "text"
      },
      "source": [
        "**The Adam variation of stochastic gradient descent is used to find the model weights. The categorical cross entropy loss function is used, required for multi-class classification, and classification accuracy is monitored during training.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0a5nMgNWRDR",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfKtXu_8WXw1",
        "colab_type": "text"
      },
      "source": [
        "**The model is fit for five training epochs and a large batch size of 128 images is used.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vQg69ksWhNd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# fit model\n",
        "model.fit(trainX, trainY, epochs=5, batch_size=128)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No4lw7KrWsmo",
        "colab_type": "text"
      },
      "source": [
        "**Once fit, the model is evaluated on the test dataset.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deJoov__Ww1k",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# evaluate model\n",
        "_, acc = model.evaluate(testX, testY, verbose=0)\n",
        "print(acc)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRdYsdpSXoH4",
        "colab_type": "text"
      },
      "source": [
        "**The complete example is listed below**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot-vgnW9Ucwj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "4faa3bfb-7ba9-4433-bd51-0551e9a86ab9"
      },
      "source": [
        "# baseline cnn model for the mnist problem\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "# load dataset\n",
        "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
        "# reshape dataset to have a single channel\n",
        "width, height, channels = trainX.shape[1], trainX.shape[2], 1\n",
        "trainX = trainX.reshape((trainX.shape[0], width, height, channels))\n",
        "testX = testX.reshape((testX.shape[0], width, height, channels))\n",
        "# normalize pixel values\n",
        "trainX = trainX.astype('float32') / 255\n",
        "testX = testX.astype('float32') / 255\n",
        "# one hot encode target values\n",
        "trainY = to_categorical(trainY)\n",
        "testY = to_categorical(testY)\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, channels)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "# compile model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(trainX, trainY, epochs=5, batch_size=128)\n",
        "# evaluate model\n",
        "_, acc = model.evaluate(testX, testY, verbose=0)\n",
        "print(acc)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "60000/60000 [==============================] - 44s 739us/step - loss: 0.2384 - acc: 0.9349\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 44s 726us/step - loss: 0.0607 - acc: 0.9818\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 44s 725us/step - loss: 0.0435 - acc: 0.9863\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 44s 730us/step - loss: 0.0345 - acc: 0.9893\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 44s 733us/step - loss: 0.0270 - acc: 0.9917\n",
            "0.9904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY6ZQ6Dpci3m",
        "colab_type": "text"
      },
      "source": [
        "**the performance of the model on the test dataset on this run is 99%, or a 1% error rate. This is not state of the art (by design), but is not terribly far from state of the art either.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3QylXpVcJy4",
        "colab_type": "text"
      },
      "source": [
        "##Step 3. Choose Pixel Scaling Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0-dqbWy4JvF",
        "colab_type": "text"
      },
      "source": [
        "**Neural network models often cannot be trained on raw pixel values, such as pixel values in the range of 0 to 255.**\n",
        "\n",
        "**The reason is that the network uses a weighted sum of inputs, and for the network to both be stable and train effectively, weights should be kept small.**\n",
        "\n",
        "**Instead, the pixel values must be scaled prior to training. There are perhaps three main approaches to scaling pixel values; they are:**\n",
        "\n",
        "**Normalization: pixel values are scaled to the range 0-1.**\n",
        "\n",
        "**Centering: the mean pixel value is subtracted from each pixel value resulting in a distribution of pixel values centered on a mean of zero.**\n",
        "\n",
        "**Standardization: the pixel values are scaled to a standard Gaussian with a mean of zero and a standard deviation of one.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8VAG9Wt4iPm",
        "colab_type": "text"
      },
      "source": [
        "**Traditionally, sigmoid activation functions were used and inputs that sum to 0 (zero mean) were preferred. This may or may not still be the case with the wide adoption of ReLU and similar activation functions.**\n",
        "\n",
        "**Further, in centering and standardization, the mean or mean and standard deviation can be calculated across a channel, an image, a mini-batch, or the entire training dataset. This may add additional variations on a chosen scaling method that may be evaluated.**\n",
        "\n",
        "**Normalization is often the default approach as we can assume pixel values are always in the range 0-255, making the procedure very simple and efficient to implement.**\n",
        "\n",
        "**Centering is often promoted as the preferred approach as it was used in many popular papers, although the mean can be calculated per image (global) or channel (local) and across the batch of images or the entire training dataset, and often the procedure described in a paper does not specify exactly which variation was used.**\n",
        "\n",
        "**We will experiment with the three approaches listed above, namely normalization, centering, and standardization. The mean for centering and the mean and standard deviation for standardization will be calculated across the entire training dataset.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDIre1jy4ylc",
        "colab_type": "text"
      },
      "source": [
        "**Other variations you could explore include:**\n",
        "\n",
        "**Calculating statistics for each channel (for color images).**\n",
        "\n",
        "**Calculating statistics for each image.**\n",
        "\n",
        "**Calculating statistics for each batch.**\n",
        "\n",
        "**Normalizing after centering or standardizing.**\n",
        "\n",
        "**The example below implements the three chosen pixel scaling methods and demonstrate their effect on the MNIST dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhX8GnjybLnI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "2be41c41-d53c-46bf-9b36-3958e641405c"
      },
      "source": [
        "#demonstarte pixel scaling methods on mnist dataset\n",
        "from keras.datasets import mnist\n",
        "#normalise image\n",
        "def prep_normalize(train, test):\n",
        "    #convert from integers to floats\n",
        "    train_norm = train.astype('float32')\n",
        "    test_norm = test.astype('float32')\n",
        "    #normalize to range 0-1\n",
        "    train_norm = train_norm / 255.0\n",
        "    test_norm = test_norm / 255.0\n",
        "    #return normalised images\n",
        "    return train_norm, test_norm\n",
        "\n",
        "#center images\n",
        "def prep_center(train, test):\n",
        "    #convert from integrs to floats\n",
        "    train_cent = train.astype('float32')\n",
        "    test_cent = test.astype('float32')\n",
        "    #calculate statistics\n",
        "    m = train_cent.mean()\n",
        "    #center datasets\n",
        "    train_cent = train_cent - m\n",
        "    test_cent = test_cent - m\n",
        "    #return normalized images\n",
        "    return train_cent, test_cent\n",
        "\n",
        "#standardize images\n",
        "def prep_standardize(train, test):\n",
        "    #convert from integers to floats\n",
        "    train_stan = train.astype('float32')\n",
        "    test_stan = test.astype('float32')\n",
        "    #calculate statistics\n",
        "    m = train_stan.mean()\n",
        "    s = train_stan.std()\n",
        "    #center datasets\n",
        "    train_stan = (train_stan - m) /s\n",
        "    test_stan = (test_stan - m) / s\n",
        "    #return normalized images\n",
        "    return train_stan, test_stan\n",
        "\n",
        "#load dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "#normalize\n",
        "trainX, testX = prep_normalize(train_images, test_images)\n",
        "print('normalization')\n",
        "print('Train', trainX.min(), trainX.max(), trainX.mean(), trainX.std())\n",
        "print('Test', testX.min(), testX.max(), testX.mean(), testX.std())\n",
        "#center\n",
        "trainX, testX = prep_center(train_images, test_images)\n",
        "print('center')\n",
        "print('Train', trainX.min(), trainX.max(), trainX.mean(), trainX.std())\n",
        "print('Test', testX.min(), testX.max(), testX.mean(), testX.std())\n",
        "#Standardize\n",
        "trainX, testX = prep_standardize(train_images, test_images)\n",
        "print('Standardize')\n",
        "print('Train', trainX.min(), trainX.max(), trainX.mean(), trainX.std())\n",
        "print('Test', testX.min(), testX.max(), testX.mean(), testX.std())\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "normalization\n",
            "Train 0.0 1.0 0.13066062 0.30810776\n",
            "Test 0.0 1.0 0.13251467 0.31048027\n",
            "center\n",
            "Train -33.318447 221.68155 -1.9512918e-05 78.567444\n",
            "Test -33.318447 221.68155 0.47278798 79.17245\n",
            "Standardize\n",
            "Train -0.42407447 2.8215446 -3.4560264e-07 0.9999998\n",
            "Test -0.42407447 2.8215446 0.0060174568 1.0077008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpGYCaJieTX7",
        "colab_type": "text"
      },
      "source": [
        "**Running the example first normalizes the dataset and reports the min, max, mean, and standard deviation for the train and test dataset.**\n",
        "\n",
        "**This is then repeated for the centering and standardization data preparation schemes. The results provide evidence that the scaling procedures are indeed implemented correctly.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwZlLgaOeaXd",
        "colab_type": "text"
      },
      "source": [
        "##Step 4. Run Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzXHejBzeoFB",
        "colab_type": "text"
      },
      "source": [
        "**Now that we have defined the dataset, the model, and the data preparation schemes to evaluate, we are ready to define and run the experiment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UQLarIUezMl",
        "colab_type": "text"
      },
      "source": [
        "**We will evaluate each of the three data preparation schemes and each scheme will be evaluated 10 times**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPhnoROIfPQ0",
        "colab_type": "text"
      },
      "source": [
        "**We can define a function to load the dataset afresh when needed.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciLUiS-pfI3G",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\t(trainX, trainY), (testX, testY) = mnist.load_data()\n",
        "\t# reshape dataset to have a single channel\n",
        "\twidth, height, channels = trainX.shape[1], trainX.shape[2], 1\n",
        "\ttrainX = trainX.reshape((trainX.shape[0], width, height, channels))\n",
        "\ttestX = testX.reshape((testX.shape[0], width, height, channels))\n",
        "\t# one hot encode target values\n",
        "\ttrainY = to_categorical(trainY)\n",
        "\ttestY = to_categorical(testY)\n",
        "\treturn trainX, trainY, testX, testY\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhlzFl82fXDz",
        "colab_type": "text"
      },
      "source": [
        "**We can also define a function to define and compile our model ready to fit on the problem.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fItoxHu9frps",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# define cnn model\n",
        "def define_model():\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, channels)))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(64, activation='relu'))\n",
        "\tmodel.add(Dense(10, activation='softmax'))\n",
        "\t# compile model\n",
        "\tmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\treturn model\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwhlwzoafz2v",
        "colab_type": "text"
      },
      "source": [
        "**We already have functions for preparing the pixel data for the train and test datasets.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdTh-yl1f8UF",
        "colab_type": "text"
      },
      "source": [
        "**Finally, we can define a function called repeated_evaluation() that takes the name of the data preparation function to call to prepare the data and will load the dataset and repeatedly define the model, prepare the dataset, fit, and evaluate the model. It will return a list of accuracy scores that can be used to summarize the performance of the model under the chosen data preparation scheme.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n1-peb7gF7Q",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# repeated evaluation of model with data prep scheme\n",
        "def repeated_evaluation(datapre_func, n_repeats=10):\n",
        "\t# prepare data\n",
        "\ttrainX, trainY, testX, testY = load_dataset()\n",
        "\t# repeated evaluation\n",
        "\tscores = list()\n",
        "\tfor i in range(n_repeats):\n",
        "\t\t# define model\n",
        "\t\tmodel = define_model()\n",
        "\t\t# prepare data\n",
        "\t\tprep_trainX, prep_testX = datapre_func(trainX, testX)\n",
        "\t\t# fit model\n",
        "\t\tmodel.fit(prep_trainX, trainY, epochs=5, batch_size=64, verbose=0)\n",
        "\t\t# evaluate model\n",
        "\t\t_, acc = model.evaluate(prep_testX, testY, verbose=0)\n",
        "\t\t# store result\n",
        "\t\tscores.append(acc)\n",
        "\t\tprint('> %d: %.3f' % (i, acc * 100.0))\n",
        "\treturn scores\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1noeWhAdgN3C",
        "colab_type": "text"
      },
      "source": [
        "**The repeated_evaluation() function can then be called for each of the three data preparation schemes and the mean and standard deviation of model performance under the scheme can be reported.**\n",
        "\n",
        "**We can also create a box and whisker plot to summarize and compare the distribution of accuracy scores for each scheme.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9iYNQNLgUPj",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "all_scores = list()\n",
        "# normalization\n",
        "scores = repeated_evaluation(prep_normalize)\n",
        "print('Normalization: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
        "all_scores.append(scores)\n",
        "# center\n",
        "scores = repeated_evaluation(prep_center)\n",
        "print('Centered: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
        "all_scores.append(scores)\n",
        "# standardize\n",
        "scores = repeated_evaluation(prep_standardize)\n",
        "print('Standardized: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
        "all_scores.append(scores)\n",
        "# box and whisker plots of results\n",
        "pyplot.boxplot(all_scores, labels=['norm', 'cent', 'stan'])\n",
        "pyplot.show()\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ifBOHbggexI",
        "colab_type": "text"
      },
      "source": [
        "**the complete example of running the experiment to compare pixel scaling methods on the MNIST dataset is listed below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhskvmcJ66ha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c9060d8-8076-4e17-e259-f098a9ec8ae2"
      },
      "source": [
        "# comparison of training-set based pixel scaling methods on MNIST\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        " \n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\t(trainX, trainY), (testX, testY) = mnist.load_data()\n",
        "\t# reshape dataset to have a single channel\n",
        "\twidth, height, channels = trainX.shape[1], trainX.shape[2], 1\n",
        "\ttrainX = trainX.reshape((trainX.shape[0], width, height, channels))\n",
        "\ttestX = testX.reshape((testX.shape[0], width, height, channels))\n",
        "\t# one hot encode target values\n",
        "\ttrainY = to_categorical(trainY)\n",
        "\ttestY = to_categorical(testY)\n",
        "\treturn trainX, trainY, testX, testY\n",
        " \n",
        "# define cnn model\n",
        "def define_model():\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(64, activation='relu'))\n",
        "\tmodel.add(Dense(10, activation='softmax'))\n",
        "\t# compile model\n",
        "\tmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\treturn model\n",
        " \n",
        "# normalize images\n",
        "def prep_normalize(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t# return normalized images\n",
        "\treturn train_norm, test_norm\n",
        " \n",
        "# center images\n",
        "def prep_center(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_cent = train.astype('float32')\n",
        "\ttest_cent = test.astype('float32')\n",
        "\t# calculate statistics\n",
        "\tm = train_cent.mean()\n",
        "\t# center datasets\n",
        "\ttrain_cent = train_cent - m\n",
        "\ttest_cent = test_cent - m\n",
        "\t# return normalized images\n",
        "\treturn train_cent, test_cent\n",
        " \n",
        "# standardize images\n",
        "def prep_standardize(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_stan = train.astype('float32')\n",
        "\ttest_stan = test.astype('float32')\n",
        "\t# calculate statistics\n",
        "\tm = train_stan.mean()\n",
        "\ts = train_stan.std()\n",
        "\t# center datasets\n",
        "\ttrain_stan = (train_stan - m) / s\n",
        "\ttest_stan = (test_stan - m) / s\n",
        "\t# return normalized images\n",
        "\treturn train_stan, test_stan\n",
        " \n",
        "# repeated evaluation of model with data prep scheme\n",
        "def repeated_evaluation(datapre_func, n_repeats=10):\n",
        "\t# prepare data\n",
        "\ttrainX, trainY, testX, testY = load_dataset()\n",
        "\t# repeated evaluation\n",
        "\tscores = list()\n",
        "\tfor i in range(n_repeats):\n",
        "\t\t# define model\n",
        "\t\tmodel = define_model()\n",
        "\t\t# prepare data\n",
        "\t\tprep_trainX, prep_testX = datapre_func(trainX, testX)\n",
        "\t\t# fit model\n",
        "\t\tmodel.fit(prep_trainX, trainY, epochs=5, batch_size=64, verbose=0)\n",
        "\t\t# evaluate model\n",
        "\t\t_, acc = model.evaluate(prep_testX, testY, verbose=0)\n",
        "\t\t# store result\n",
        "\t\tscores.append(acc)\n",
        "\t\tprint('> %d: %.3f' % (i, acc * 100.0))\n",
        "\treturn scores\n",
        " \n",
        "all_scores = list()\n",
        "# normalization\n",
        "scores = repeated_evaluation(prep_normalize)\n",
        "print('Normalization: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
        "all_scores.append(scores)\n",
        "# center\n",
        "scores = repeated_evaluation(prep_center)\n",
        "print('Centered: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
        "all_scores.append(scores)\n",
        "# standardize\n",
        "scores = repeated_evaluation(prep_standardize)\n",
        "print('Standardized: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
        "all_scores.append(scores)\n",
        "# box and whisker plots of results\n",
        "pyplot.boxplot(all_scores, labels=['norm', 'cent', 'stan'])\n",
        "pyplot.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "> 0: 99.190\n",
            "> 1: 98.510\n",
            "> 2: 99.050\n",
            "> 3: 98.730\n",
            "> 4: 98.890\n",
            "> 5: 98.980\n",
            "> 6: 99.050\n",
            "> 7: 99.060\n",
            "> 8: 98.990\n",
            "> 9: 99.160\n",
            "Normalization: 0.990 (0.002)\n",
            "> 0: 50.770\n",
            "> 1: 98.520\n",
            "> 2: 98.840\n",
            "> 3: 98.390\n",
            "> 4: 9.800\n",
            "> 5: 10.100\n",
            "> 6: 98.430\n",
            "> 7: 98.250\n",
            "> 8: 98.630\n",
            "> 9: 98.260\n",
            "Centered: 0.760 (0.359)\n",
            "> 0: 99.140\n",
            "> 1: 99.070\n",
            "> 2: 98.650\n",
            "> 3: 99.080\n",
            "> 4: 99.120\n",
            "> 5: 99.050\n",
            "> 6: 99.010\n",
            "> 7: 98.630\n",
            "> 8: 98.940\n",
            "> 9: 99.040\n",
            "Standardized: 0.990 (0.002)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrfiQPoGvxvk",
        "colab_type": "text"
      },
      "source": [
        "##Step 5. Analyze Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB0JyO1yv5CT",
        "colab_type": "text"
      },
      "source": [
        "**The results of the experiments show that there is little or no difference (at the chosen precision) between pixel normalization and standardization with the chosen model on the MNIST dataset.**\n",
        "\n",
        "**From these results, I would use normalization over standardization on this dataset with this model because of the good results and because of the simplicity of normalization as compared to standardization.**\n",
        "\n",
        "**These are useful results in that they show that the default heuristic to center pixel values prior to modeling would not be good advice for this dataset.**\n",
        "\n",
        "**Sadly, the box and whisker plot does not make a comparison between the spread of accuracy scores easy as some terrible outlier scores for the centering scaling method squash the distributions.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_4t1RUjn0sW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}