{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How to One Hot Encode Sequence Data in Python.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPCe2hud1T2NN8Ap9iljYOA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhbhupender1/ML-notebooks/blob/master/How_to_One_Hot_Encode_Sequence_Data_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUifq-hVn7vM",
        "colab_type": "text"
      },
      "source": [
        "#What is One Hot Encoding?\n",
        "**A one hot encoding is a representation of categorical variables as binary vectors.**\n",
        "\n",
        "**This first requires that the categorical values be mapped to integer values.**\n",
        "\n",
        "**Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT4iS6sYulzK",
        "colab_type": "text"
      },
      "source": [
        "##Why Use a One Hot Encoding?\n",
        "**A one hot encoding allows the representation of categorical data to be more expressive.**\n",
        "\n",
        "**Many machine learning algorithms cannot work with categorical data directly. The categories must be converted into numbers. This is required for both input and output variables that are categorical.**\n",
        "\n",
        "**We could use an integer encoding directly, rescaled where needed. This may work for problems where there is a natural ordinal relationship between the categories, and in turn the integer values, such as labels for temperature ‘cold’, warm’, and ‘hot’.**\n",
        "\n",
        "**There may be problems when there is no ordinal relationship and allowing the representation to lean on any such relationship might be damaging to learning to solve the problem. An example might be the labels ‘dog’ and ‘cat’**\n",
        "\n",
        "**In these cases, we would like to give the network more expressive power to learn a probability-like number for each possible label value. This can help in both making the problem easier for the network to model. When a one hot encoding is used for the output variable, it may offer a more nuanced set of predictions than a single label.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k_O-KySv8X_",
        "colab_type": "text"
      },
      "source": [
        "##Manual One Hot Encoding\n",
        "**In this example, we will assume the case where we have an example string of characters of alphabet letters, but the example sequence does not cover all possible examples.**\n",
        "\n",
        "**We will use the input sequence of the following characters:**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "hello world\n",
        "```\n",
        "**The complete example is listed below.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG-o-MOEkELt",
        "colab_type": "code",
        "outputId": "b854012c-586c-477c-d48a-d673efcdd25e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "from numpy import argmax\n",
        "# define input string\n",
        "data = 'hello world'\n",
        "print(data)\n",
        "# define universe of possible input values\n",
        "alphabet = 'abcdefghijklmnopqrstuvwxyz '\n",
        "# define a mapping of chars to integers\n",
        "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
        "# integer encode input data\n",
        "integer_encoded = [char_to_int[char] for char in data]\n",
        "print(integer_encoded)\n",
        "# one hot encode\n",
        "onehot_encoded = list()\n",
        "for value in integer_encoded:\n",
        "\tletter = [0 for _ in range(len(alphabet))]\n",
        "\tletter[value] = 1\n",
        "\tonehot_encoded.append(letter)\n",
        "print(onehot_encoded)\n",
        "# invert encoding\n",
        "inverted = int_to_char[argmax(onehot_encoded[0])]\n",
        "print(inverted)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello world\n",
            "[7, 4, 11, 11, 14, 26, 22, 14, 17, 11, 3]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "h\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZr6PC9C1aaW",
        "colab_type": "text"
      },
      "source": [
        "**A mapping of all possible inputs is created from char values to integer values. This mapping is then used to encode the input string. We can see that the first letter in the input ‘h’ is encoded as 7, or the index 7 in the array of possible input values (alphabet).**\n",
        "\n",
        "**The integer encoding is then converted to a one hot encoding. This is done one integer encoded character at a time. A list of 0 values is created the length of the alphabet so that any expected character can be represented.**\n",
        "\n",
        "**Next, the index of the specific character is marked with a 1. We can see that the first letter ‘h’ integer encoded as a 7 is represented by a binary vector with the length 27 and the 7th index marked with a 1.**\n",
        "\n",
        "**Finally, we invert the encoding of the first letter and print the result. We do this by locating the index of in the binary vector with the largest value using the NumPy argmax() function and then using the integer value in a reverse lookup table of character values to integers.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "346DH5ex14WM",
        "colab_type": "text"
      },
      "source": [
        "##One Hot Encode with scikit-learn\n",
        "\n",
        "**This would first require an integer encoding, such as 1, 2, 3. This would be followed by a one hot encoding of integers to a binary vector with 3 values, such as [1, 0, 0].**\n",
        "\n",
        "**The sequence provides at least one example of every possible value in the sequence. Therefore we can use automatic methods to define the mapping of labels to integers and integers to binary vectors.**\n",
        "\n",
        "**In this example, we will use the encoders from the scikit-learn library. Specifically, the LabelEncoder of creating an integer encoding of labels and the OneHotEncoder for creating a one hot encoding of integer encoded values.**\n",
        "\n",
        "**The complete example is listed below.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFqMUe1H1lAK",
        "colab_type": "code",
        "outputId": "b64cd208-65cb-4491-dafc-dd3bfd299b59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "#define example\n",
        "data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']\n",
        "values = array(data)\n",
        "print(values)\n",
        "#integer values\n",
        "label_encoder = LabelEncoder()\n",
        "integer_encoded = label_encoder.fit_transform(values)\n",
        "print(integer_encoded)\n",
        "#binary encode\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
        "print(onehot_encoded)\n",
        "#invert first example\n",
        "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
        "print(inverted)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['cold' 'cold' 'warm' 'cold' 'hot' 'hot' 'warm' 'cold' 'warm' 'hot']\n",
            "[0 0 2 0 1 1 2 0 2 1]\n",
            "[[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]]\n",
            "['cold']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmhmZ3Zn4KVo",
        "colab_type": "text"
      },
      "source": [
        "**Running the example first prints the sequence of labels. This is followed by the integer encoding of the labels and finally the one hot encoding.**\n",
        "\n",
        "**The training data contained the set of all possible examples so we could rely on the integer and one hot encoding transforms to create a complete mapping of labels to encodings.**\n",
        "\n",
        "**By default, the OneHotEncoder class will return a more efficient sparse encoding. This may not be suitable for some applications, such as use with the Keras deep learning library. In this case, we disabled the sparse return type by setting the sparse=False argument.**\n",
        "\n",
        "**If we receive a prediction in this 3-value one hot encoding, we can easily invert the transform back to the original label.**\n",
        "\n",
        "**First, we can use the argmax() NumPy function to locate the index of the column with the largest value. This can then be fed to the LabelEncoder to calculate an inverse transform back to a text label.**\n",
        "\n",
        "**This is demonstrated at the end of the example with the inverse transform of the first one hot encoded example back to the label value ‘cold’.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0ShfiGCDg2F",
        "colab_type": "text"
      },
      "source": [
        "##**One Hot Encode with Keras**\n",
        "\n",
        "**You may have a sequence that is already integer encoded.**\n",
        "\n",
        "**You could work with the integers directly, after some scaling. Alternately, you can one hot encode the integers directly. This is important to consider if the integers do not have a real ordinal relationship and are really just placeholders for labels.**\n",
        "\n",
        "**The Keras library offers a function called to_categorical() that you can use to one hot encode integer data.**\n",
        "\n",
        "**In this example, we have 4 integer values [0, 1, 2, 3] and we have the input sequence of the following 10 numbers:**\n",
        "\n",
        "**The sequence has an example of all known values so we can use the to_categorical() function directly. Alternately, if the sequence was 0-based (started at 0) and was not representative of all possible values, we could specify the num_classes argument to_categorical(num_classes=4).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFDpjFH13-NB",
        "colab_type": "code",
        "outputId": "394c16d0-17f1-4fa0-e67e-dfe47ee8b868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.utils import to_categorical\n",
        "#define example\n",
        "data = [1, 3, 2, 0, 3, 2, 2, 1, 0, 1]\n",
        "data = array(data)\n",
        "print(data)\n",
        "#one hot encode\n",
        "encoded = to_categorical(data)\n",
        "print(encoded)\n",
        "#invert encoding\n",
        "inverted = argmax(encoded[0])\n",
        "print(inverted)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 3 2 0 3 2 2 1 0 1]\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqSRDjizEwXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}