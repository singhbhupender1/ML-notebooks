{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro to ADAM optimization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPPLg1ivAPB8zNZcJMJpM+Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhbhupender1/ML-notebooks/blob/master/Intro_to_ADAM_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oadHbSu0WEl",
        "colab_type": "text"
      },
      "source": [
        "##the attractive benefits of using Adam on non-convex optimization problems, as follows:\n",
        "\n",
        "**Straightforward to implement.**\n",
        "\n",
        "**Computationally efficient.**\n",
        "\n",
        "**Little memory requirements.**\n",
        "\n",
        "**Invariant to diagonal rescale of the gradients.**\n",
        "\n",
        "**Well suited for problems that are large in terms of data and/or parameters.\n",
        "Appropriate for non-stationary objectives.**\n",
        "\n",
        "**Appropriate for problems with very noisy/or sparse gradients.**\n",
        "\n",
        "**Hyper-parameters have intuitive interpretation and typically require little tuning.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk3y5aYNxSC8",
        "colab_type": "text"
      },
      "source": [
        "##Adam Configuration Parameters\n",
        "**alphA. Also referred to as the learning rate or step size. The proportion that weights are updated (e.g. 0.001). Larger values (e.g. 0.3) results in faster initial learning before the rate is updated. Smaller values (e.g. 1.0E-5) slow learning right down during training**\n",
        "\n",
        "**beta1. The exponential decay rate for the first moment estimates (e.g. 0.9).**\n",
        "\n",
        "**beta2. The exponential decay rate for the second-moment estimates (e.g. 0.999). This value should be set close to 1.0 on problems with a sparse gradient (e.g. NLP and computer vision problems).**\n",
        "\n",
        "**epsilon. Is a very small number to prevent any division by zero in the implementation (e.g. 10E-8).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYhitteoxuFz",
        "colab_type": "text"
      },
      "source": [
        "##We can see that the popular deep learning libraries generally use the default parameters recommended by the paper.\n",
        "\n",
        "**TensorFlow: learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08.**\n",
        "\n",
        "**Keras: lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0.\n",
        "Blocks: learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-08, decay_factor=1.**\n",
        "\n",
        "**Lasagne: learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08\n",
        "Caffe: learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08**\n",
        "\n",
        "**MxNet: learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8\n",
        "Torch: learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SybaS1IWxRAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}